{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466107fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filip\\Projects\\Neural_IR_Expansion\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2438834",
   "metadata": {},
   "source": [
    "# Data loading and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9836da",
   "metadata": {},
   "source": [
    "Since I was having problems with the ir_datasets library, I downloaded locally the dataset files from https://ciir.cs.umass.edu/downloads/Antique/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e86372",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5124a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\filip\\Projects\\Neural_IR_Expansion\\Dataset\"\n",
    "\n",
    "antique_collection = os.path.join(dataset_path, \"antique-collection.txt\")\n",
    "docs_df_cols = ['answer_id', 'answer_text']\n",
    "docs = pd.read_csv(antique_collection, sep='\\t', header=None, names=docs_df_cols)\n",
    "\n",
    "antique_train_queries = os.path.join(dataset_path, \"antique-train-queries.txt\")\n",
    "train_queries_cols = [\"query_id\", \"query_text\"]\n",
    "train_queries = pd.read_csv(antique_train_queries, sep='\\t', header=None, names=train_queries_cols)\n",
    "\n",
    "antique_train_qrels = os.path.join(dataset_path, \"antique-train.qrel.txt\")\n",
    "train_qrels_cols = [\"query_id\", \"relevance_type\", \"answer_id\", \"relevance\"]\n",
    "train_qrels = pd.read_csv(antique_train_qrels, sep=r'\\s+', header=None, names=train_qrels_cols) # the sep handles the separation between columns \" \" and \"\\t\"\n",
    "\n",
    "antique_test_queries = os.path.join(dataset_path, \"antique-test-queries.txt\")\n",
    "test_queries_cols = [\"query_id\", \"query_text\"]\n",
    "test_queries = pd.read_csv(antique_test_queries, sep='\\t', header=None, names=test_queries_cols)\n",
    "\n",
    "antique_test_qrels = os.path.join(dataset_path, \"antique-test.qrel.txt\")\n",
    "test_qrels_cols = [\"query_id\", \"relevance_type\", \"answer_id\", \"relevance\"]\n",
    "test_qrels = pd.read_csv(antique_test_qrels, sep=r'\\s+', header=None, names=test_qrels_cols) # the sep handles the separation between columns \" \" and \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b56b8",
   "metadata": {},
   "source": [
    "From the dataset readme, we know that the \"doc_id\" column contains two separate informations, so we can split the \"doc_id\" column to get the \"query_id\" and \"answer_n\" separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[['query_id', 'answer_n']] = docs['answer_id'].str.split('_', expand=True)\n",
    "train_qrels[['query_id', 'answer_n']] = train_qrels['answer_id'].str.split('_', expand=True)\n",
    "test_qrels[['query_id', 'answer_n']] = test_qrels['answer_id'].str.split('_', expand=True)\n",
    "\n",
    "docs['query_id'] = docs['query_id'].astype(int)\n",
    "train_qrels['query_id'] = train_qrels['query_id'].astype(int)\n",
    "test_qrels['query_id'] = test_qrels['query_id'].astype(int)\n",
    "\n",
    "docs['answer_n'] = docs['answer_n'].astype(int)\n",
    "train_qrels['answer_n'] = train_qrels['answer_n'].astype(int)\n",
    "test_qrels['answer_n'] = test_qrels['answer_n'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc65e7",
   "metadata": {},
   "source": [
    "Now, let's see how our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c888e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs shape: (403458, 4)\n",
      "train_queries shape: (2426, 2)\n",
      "train_qrels shape: (27422, 5)\n",
      "test_queries shape: (200, 2)\n",
      "test_qrels shape: (6589, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"docs shape: {docs.shape}\")\n",
    "print(f\"train_queries shape: {train_queries.shape}\")\n",
    "print(f\"train_qrels shape: {train_qrels.shape}\")\n",
    "print(f\"test_queries shape: {test_queries.shape}\")\n",
    "print(f\"test_qrels shape: {test_qrels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffbf5e",
   "metadata": {},
   "source": [
    "### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81dc8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_id      0\n",
      "answer_text    5\n",
      "query_id       0\n",
      "answer_n       0\n",
      "dtype: int64\n",
      "\n",
      "query_id      0\n",
      "query_text    0\n",
      "dtype: int64\n",
      "\n",
      "query_id          0\n",
      "relevance_type    0\n",
      "answer_id         0\n",
      "relevance         0\n",
      "answer_n          0\n",
      "dtype: int64\n",
      "\n",
      "query_id      0\n",
      "query_text    0\n",
      "dtype: int64\n",
      "\n",
      "query_id          0\n",
      "relevance_type    0\n",
      "answer_id         0\n",
      "relevance         0\n",
      "answer_n          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs.isnull().sum()}\\n\")\n",
    "print(f\"{train_queries.isnull().sum()}\\n\")\n",
    "print(f\"{train_qrels.isnull().sum()}\\n\")\n",
    "print(f\"{test_queries.isnull().sum()}\\n\")\n",
    "print(test_qrels.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581e563",
   "metadata": {},
   "source": [
    "We have 1 column in docs (\"text\") with 5 null values. It probably corresponds to some uploaded but empty answers by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4b2abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>query_id</th>\n",
       "      <th>answer_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>1940280_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1940280</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31936</th>\n",
       "      <td>435733_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35357</th>\n",
       "      <td>3975689_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3975689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140950</th>\n",
       "      <td>2677531_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2677531</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262306</th>\n",
       "      <td>493662_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493662</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        answer_id answer_text  query_id  answer_n\n",
       "5378    1940280_3         NaN   1940280         3\n",
       "31936    435733_1         NaN    435733         1\n",
       "35357   3975689_3         NaN   3975689         3\n",
       "140950  2677531_4         NaN   2677531         4\n",
       "262306   493662_4         NaN    493662         4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_nulls = docs[docs.isnull().any(axis=1)]\n",
    "docs_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6061555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>relevance_type</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>relevance</th>\n",
       "      <th>answer_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2677531</td>\n",
       "      <td>E0</td>\n",
       "      <td>2677531_4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id relevance_type  answer_id  relevance  answer_n\n",
       "0   2677531             E0  2677531_4          3         4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_qrels = train_qrels.merge(docs_nulls[['query_id', 'answer_id']], on=['query_id', 'answer_id'])\n",
    "matching_qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb036517",
   "metadata": {},
   "source": [
    "Let's get rid of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1dbe12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>relevance_type</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>relevance</th>\n",
       "      <th>answer_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [query_id, relevance_type, answer_id, relevance, answer_n]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qrels = train_qrels[train_qrels['answer_id'] != \"2677531_4\"]\n",
    "\n",
    "matching_qrels = train_qrels.merge(docs_nulls[['query_id', 'answer_id']], on=['query_id', 'answer_id'])\n",
    "matching_qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa554e9",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42bef66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "\n",
      "docs: 0\n",
      "train_queries: 0\n",
      "train_qrels: 0\n",
      "test_queries: 0\n",
      "test_qrels: 35\n"
     ]
    }
   ],
   "source": [
    "print(\"Duplicate rows:\\n\")\n",
    "print(\"docs:\", docs.duplicated().sum())\n",
    "print(\"train_queries:\", train_queries.duplicated().sum())\n",
    "print(\"train_qrels:\", train_qrels.duplicated().sum())\n",
    "print(\"test_queries:\", test_queries.duplicated().sum())\n",
    "print(\"test_qrels:\", test_qrels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8297ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>relevance_type</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>relevance</th>\n",
       "      <th>answer_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>144229</td>\n",
       "      <td>Q0</td>\n",
       "      <td>144229_5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>144229</td>\n",
       "      <td>Q0</td>\n",
       "      <td>144229_5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>229566</td>\n",
       "      <td>Q0</td>\n",
       "      <td>229566_0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>229566</td>\n",
       "      <td>Q0</td>\n",
       "      <td>229566_0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>230487</td>\n",
       "      <td>Q0</td>\n",
       "      <td>230487_0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>4169123</td>\n",
       "      <td>Q0</td>\n",
       "      <td>4169123_11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>4225209</td>\n",
       "      <td>Q0</td>\n",
       "      <td>4225209_3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>4225209</td>\n",
       "      <td>Q0</td>\n",
       "      <td>4225209_3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>4235114</td>\n",
       "      <td>Q0</td>\n",
       "      <td>4235114_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>4235114</td>\n",
       "      <td>Q0</td>\n",
       "      <td>4235114_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id relevance_type   answer_id  relevance  answer_n\n",
       "1351    144229             Q0    144229_5          1         5\n",
       "3525    144229             Q0    144229_5          1         5\n",
       "3248    229566             Q0    229566_0          2         0\n",
       "4376    229566             Q0    229566_0          2         0\n",
       "1546    230487             Q0    230487_0          4         0\n",
       "...        ...            ...         ...        ...       ...\n",
       "5387   4169123             Q0  4169123_11          1        11\n",
       "1870   4225209             Q0   4225209_3          2         3\n",
       "3090   4225209             Q0   4225209_3          2         3\n",
       "1851   4235114             Q0   4235114_1          1         1\n",
       "2968   4235114             Q0   4235114_1          1         1\n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qrels_duplicates = test_qrels[test_qrels.duplicated(keep=False)].sort_values(by=test_qrels.columns.tolist())\n",
    "test_qrels_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a94e5",
   "metadata": {},
   "source": [
    "Let's drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238fec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_qrels duplicates: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qrels.drop_duplicates(inplace=True)\n",
    "print(\"test_qrels duplicates:\", test_qrels.duplicated().sum())\n",
    "len(test_qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bda61",
   "metadata": {},
   "source": [
    "### Merge the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dad83",
   "metadata": {},
   "source": [
    "Now we want to merge the datasets in order to have only two for train and test (we will extract also a validation set from the train).\n",
    "\n",
    "So, let's check if all the query_id's in the query df's for train and test appear also in the qrels df's, and if so we can start merging these two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b193e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All train_queries in train_qrels? True\n",
      "Train qrels query_ids NOT in train_queries: 0\n"
     ]
    }
   ],
   "source": [
    "# Train: Check if all query_ids in train_queries are in train_qrels\n",
    "train_queries_in_qrels = set(train_queries['query_id']).issubset(set(train_qrels['query_id']))\n",
    "print(\"All train_queries in train_qrels?\", train_queries_in_qrels)\n",
    "\n",
    "# Check if there are query_ids in qrels not in queries\n",
    "train_qrels_extra_queries = set(train_qrels['query_id']) - set(train_queries['query_id'])\n",
    "print(\"Train qrels query_ids NOT in train_queries:\", len(train_qrels_extra_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276fc32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test_queries in test_qrels? True\n",
      "Test qrels query_ids NOT in test_queries: 3676\n"
     ]
    }
   ],
   "source": [
    "# Train: Check if all query_ids in train_queries are in train_qrels\n",
    "test_queries_in_qrels = set(test_queries['query_id']).issubset(set(test_qrels['query_id']))\n",
    "print(\"All test_queries in test_qrels?\", test_queries_in_qrels)\n",
    "\n",
    "# Check if there are query_ids in qrels not in queries\n",
    "test_qrels_extra_queries = set(test_qrels['query_id']) - set(test_queries['query_id'])\n",
    "print(\"Test qrels query_ids NOT in test_queries:\", len(test_qrels_extra_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e170",
   "metadata": {},
   "source": [
    "We have some queries in the test qrels df that are not present in the test queries df, so let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe9600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6554\n",
      "2181\n",
      "Test qrels query_ids NOT in test_queries: 0\n"
     ]
    }
   ],
   "source": [
    "print(len(test_qrels))\n",
    "test_qrels = test_qrels[test_qrels['query_id'].isin(test_queries['query_id'])]\n",
    "print(len(test_qrels))\n",
    "\n",
    "test_qrels_extra_queries = set(test_qrels['query_id']) - set(test_queries['query_id'])\n",
    "print(\"Test qrels query_ids NOT in test_queries:\", len(test_qrels_extra_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97184c",
   "metadata": {},
   "source": [
    "Now we can merge queries and qrels for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efb616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.merge(train_qrels, train_queries, on='query_id', how='inner')\n",
    "test_merged = pd.merge(test_qrels, test_queries, on='query_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8301df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_merged shape: (27421, 6)\n",
      "test_merged shape: (2181, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_merged shape: {train_merged.shape}\")\n",
    "print(f\"test_merged shape: {test_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4741a",
   "metadata": {},
   "source": [
    "Check if all answer_ids in the merged DataFrames are in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e042f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: answer_ids not in docs: 16\n",
      "Test: answer_ids not in docs: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for train\n",
    "missing_train_answers = ~train_merged['answer_id'].isin(docs['answer_id'])\n",
    "print(\"Train: answer_ids not in docs:\", missing_train_answers.sum())\n",
    "\n",
    "# Check for test\n",
    "missing_test_answers = ~test_merged['answer_id'].isin(docs['answer_id'])\n",
    "print(\"Test: answer_ids not in docs:\", missing_test_answers.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64b997",
   "metadata": {},
   "source": [
    "Let's drop rows with missing answer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d15992b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27405"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged = train_merged[train_merged['answer_id'].isin(docs['answer_id'])]\n",
    "len(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde8406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final merge\n",
    "train_val_df = pd.merge(train_merged, docs[['answer_id', 'answer_text']], on='answer_id', how='inner')\n",
    "test_df = pd.merge(test_merged, docs[['answer_id', 'answer_text']], on='answer_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9777a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df['relevance'] = (train_val_df['relevance'] - 1) / 3\n",
    "test_df['relevance'] = (test_df['relevance'] - 1) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a1e52",
   "metadata": {},
   "source": [
    "Now we want also to extract the validation set from train_df. We will do a 10% split.  \n",
    "\n",
    "Splitting by query_id guarantees no overlap of queries between train and validation, altho it won't be a perfect 10% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05bb131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split shape: (24685, 7)\n",
      "Validation split shape: (2720, 7)\n",
      "Test split shape: (2181, 7)\n"
     ]
    }
   ],
   "source": [
    "unique_queries = train_val_df['query_id'].unique()\n",
    "split_idx = int(len(unique_queries) * 0.1)  # 10% for validation\n",
    "\n",
    "# Split query IDs\n",
    "val_queries = unique_queries[:split_idx]\n",
    "train_queries = unique_queries[split_idx:]\n",
    "\n",
    "# Split the dataframe accordingly\n",
    "train_df = train_val_df[train_val_df['query_id'].isin(train_queries)].reset_index(drop=True)\n",
    "val_df = train_val_df[train_val_df['query_id'].isin(val_queries)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train split shape: {train_df.shape}\")\n",
    "print(f\"Validation split shape: {val_df.shape}\")\n",
    "print(f\"Test split shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f78bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id          0\n",
      "relevance_type    0\n",
      "answer_id         0\n",
      "relevance         0\n",
      "answer_n          0\n",
      "query_text        0\n",
      "answer_text       0\n",
      "dtype: int64\n",
      "query_id          0\n",
      "relevance_type    0\n",
      "answer_id         0\n",
      "relevance         0\n",
      "answer_n          0\n",
      "query_text        0\n",
      "answer_text       0\n",
      "dtype: int64\n",
      "query_id          0\n",
      "relevance_type    0\n",
      "answer_id         0\n",
      "relevance         0\n",
      "answer_n          0\n",
      "query_text        0\n",
      "answer_text       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())\n",
    "print(val_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588c125",
   "metadata": {},
   "source": [
    "We have our clean data!  \n",
    "Let's save it, so that it will be easily reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd34692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"data/MyData/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/MyData/val.csv\", index=False)\n",
    "test_df.to_csv(\"data/MyData/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc73558",
   "metadata": {},
   "source": [
    "# Baseline Retrieval: implement and analyse BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67557a6c",
   "metadata": {},
   "source": [
    "BM25 ranks the answer_texts for each query_text. We will perform a hyperparameter optimization on BM25's 2 parameters k1 and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bacb42",
   "metadata": {},
   "source": [
    "Preprocessing  :\n",
    "\n",
    "BM25 works with tokens (words). So for both queries and answers, we should:\n",
    "\n",
    "- Lowercase text\n",
    "\n",
    "- Remove punctuation\n",
    "\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c741d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bb2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_scores(query_tokens, answers_tokens, k1=1.8, b=0.3):\n",
    "    bm25 = BM25Okapi(answers_tokens, k1=k1, b=b)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36543f",
   "metadata": {},
   "source": [
    "We'll use **NDCG@k** (Normalized Discounted Cumulative Gain) as metric for hyperparameter optimization\n",
    "- k=None returns all answers. If I set a num, it will return the top k answers\n",
    "- ndcg_score will internally compute DCG and normalize it by the ideal DCG, so we dont need to normalize true_relevance (which right now is between 1 and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cfa71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bm25(df, k1, b):\n",
    "    ndcgs = []\n",
    "\n",
    "    for query_id, group in df.groupby(\"query_id\"):\n",
    "        query = group[\"query_text\"].iloc[0]\n",
    "        query_input = preprocess(query)\n",
    "\n",
    "        answers = group[\"answer_text\"]\n",
    "        answers_input = [preprocess(ans) for ans in answers]\n",
    "\n",
    "        true_relevance = group[\"relevance\"].tolist()\n",
    "\n",
    "        scores = bm25_scores(query_input, answers_input, k1, b).tolist()\n",
    "        ndcgs.append(ndcg_score([true_relevance], [scores], k=None))\n",
    "\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53598315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 18:32:08,750] A new study created in memory with name: no-name-73254317-12ce-45b4-a4dd-617de9e9be68\n",
      "Best trial: 47. Best value: 0.931129: 100%|██████████| 50/50 [02:20<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'k1': 1.833306339035504, 'b': 0.3000843692671696}\n",
      "Best NDCG score: 0.9311293095373528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    k1 = trial.suggest_float(\"k1\", 0.5, 2.0)\n",
    "    b = trial.suggest_float(\"b\", 0.3, 1.0)\n",
    "    \n",
    "    return evaluate_bm25(train_df, k1, b)\n",
    "\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best NDCG score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e978415",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'k1': 1.833306339035504, 'b': 0.3000843692671696}\n",
    "Best NDCG score: 0.9311293095373528\n",
    "\n",
    "We will be using k1=1.8 and b=0.3\n",
    "\n",
    "Our NDCG score is already really good. Let's try to make it even better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71198c0e",
   "metadata": {},
   "source": [
    "# CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26274590",
   "metadata": {},
   "source": [
    "## CrossEncoder fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "035e1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = train_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e95b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [\n",
    "    InputExample(texts=[row[\"query_text\"], row[\"answer_text\"]], label=float(row[\"relevance\"]))\n",
    "    for _, row in train_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd018570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (867 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15430' max='15430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15430/15430 17:29:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.454800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", num_labels=1)\n",
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "\n",
    "folder_path = r\"C:\\Users\\filip\\Projects\\Neural_IR_Expansion\"\n",
    "experiment_name = \"crossencoder_fineTuning_long\"\n",
    "destination_path = os.path.join(folder_path, experiment_name)\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    ")\n",
    "model.save(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12bf9b",
   "metadata": {},
   "source": [
    "# Creation of the expansion dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_file = r\"C:\\Users\\filip\\Projects\\Neural_IR_Expansion\\data\\Expansions\\expert_user_expansion.txt\"\n",
    "with open(expert_file, \"r\") as f:\n",
    "    lines = [line for line in f.readlines() if line.strip()]\n",
    "    \n",
    "expert_queries = []\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith(\"Rewritten: \"):\n",
    "        expert_queries.append(line.replace(\"Rewritten: \", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "\n",
    "novice_file = r\"C:\\Users\\filip\\Projects\\Neural_IR_Expansion\\data\\Expansions\\novice_user_expansion.txt\"\n",
    "with open(novice_file, \"r\") as f:\n",
    "    lines = [line for line in f.readlines() if line.strip()]\n",
    "    \n",
    "novice_queries = []\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith(\"Rewritten: \"):\n",
    "        novice_queries.append(line.replace(\"Rewritten: \", \"\").replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(novice_queries))\n",
    "print(len(expert_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bd6b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"data/MyData/test.csv\")\n",
    "unique_queries = test_df['query_text'].unique()\n",
    "\n",
    "mapping_novice = dict(zip(unique_queries, novice_queries))\n",
    "novice_df = test_df.copy()\n",
    "novice_df['query_text'] = novice_df['query_text'].map(mapping_novice)\n",
    "\n",
    "mapping_expert = dict(zip(unique_queries, expert_queries))\n",
    "expert_df = test_df.copy()\n",
    "expert_df['query_text'] = expert_df['query_text'].map(mapping_expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35491245",
   "metadata": {},
   "outputs": [],
   "source": [
    "novice_df.to_csv(\"data/MyData/novice_expansion.csv\", index=False)\n",
    "expert_df.to_csv(\"data/MyData/expert_expansion.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neural_IR_Expansion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
